{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "040e57d0",
   "metadata": {},
   "source": [
    "- Tokenize input.\n",
    "- Create a label with input or clone input tensor.\n",
    "- Randomly masked some token in input.\n",
    "- Initialize the model and calculate the loss.\n",
    "- Finally update weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71137cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_basic_tokenization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426afd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collate_fn(dataset_samples_list):\n",
    "    # Makes the dataset a numpy array instead of a python list\n",
    "    arr = np.array(dataset_samples_list)\n",
    "    \n",
    "    # Tokenizes the data using the BERT tokenizer\n",
    "    inputs = tokenizer(text=arr.tolist(), padding='max_length', max_length=30, return_tensors='pt')\n",
    "    return inputs\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer):\n",
    "        self.txt = txt\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.txt)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.txt[idx]\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587dd842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        # Initializes the parameters od the model\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # Initializes model type\n",
    "        self.model_type = 'Transformer'\n",
    "        # Gets the positional encoder \n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        # Sets the laters based on the hyperparameters\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout, batch_first=True)\n",
    "        # \n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # Creates an attention mask with zeros across the diagonal\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initializes the weights of the model between the modes before training the dataset\n",
    "        # Sets the range in which the values of the weights can occur\n",
    "        initrange = 0.1\n",
    "        # Sets the weights of the encoder\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # Zeros the weights of the decoder\n",
    "        self.decoder.bias.data.zero_()\n",
    "        # Sets the weights of the decoder\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, txt, txt_mask):\n",
    "        txt = self.encoder(txt) * math.sqrt(self.ninp)\n",
    "        txt = self.pos_encoder(txt)\n",
    "        output = self.transformer_encoder(txt, txt_mask)\n",
    "        # Decodes the processed output\n",
    "        output = self.decoder(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea4beb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    epochs = 500\n",
    "    total_loss = 0\n",
    "    # Sets how loss will be calcuated using Cross Entropy\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Sets the optimizer to be AdamW\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            # Zeroes the gradient used for gradient descent\n",
    "            optimizer.zero_grad()\n",
    "            # Gets the input ids from the batch of the dataset\n",
    "            input = batch['input_ids'].clone()\n",
    "            # Generates a square subsequent mask of the batch of the dataset\n",
    "            txt_mask = model.generate_square_subsequent_mask(batch['input_ids'].size(1))\n",
    "            # Creates an array the size of input id's to mask out 15% of the dataset\n",
    "            rand_value = torch.rand(batch.input_ids.shape)\n",
    "            # This adds parameters to the random mask to make sure special tokens do not get masked\n",
    "            rand_mask = (rand_value < 0.15) * (input != 101) * (input != 102) * (input != 0)\n",
    "            # This gets the indexes of all of the tokens to be masked from the dataset\n",
    "            mask_idx = (rand_mask.flatten() == True).nonzero().view(-1)\n",
    "            # The inputs are flattened for multiplication\n",
    "            input = input.flatten()\n",
    "            # Sets which token is the [MASK] token\n",
    "            input[mask_idx] = 103\n",
    "            input = input.view(batch['input_ids'].size())\n",
    "            \n",
    "            # Gets the output by entering the masked input and unmasked input into the model infrastructure\n",
    "            out = model(input.to(device), txt_mask.to(device))\n",
    "            # Calculated the loss of the model (-log p(word|sentence))\n",
    "            loss = criterion(out.view(-1, ntokens), batch['input_ids'].view(-1).to(device))\n",
    "            # Adds the loss of each batch to the total loss\n",
    "            total_loss += loss\n",
    "            # Does backward propagation\n",
    "            loss.backward()\n",
    "            # Makes the optimizer take a step\n",
    "            optimizer.step()\n",
    "    \n",
    "        # Prints the total loss and which step the model is on every 40 epochs\n",
    "        if (epoch+1)%40==0 or epoch==0:\n",
    "            print(\"Epoch: {} -> loss: {}\".format(epoch+1, total_loss/(len(dataloader)*epoch+1)))\n",
    "            \n",
    "def predict(model, input):\n",
    "    # Uses model.eval to get\n",
    "    model.eval()\n",
    "    # Generates a square subsequent mask for the example input\n",
    "    txt_mask = model.generate_square_subsequent_mask(input.size(1))\n",
    "    # Inputs the square subsequent mask of the input and the input into the model\n",
    "    out = model(input.to(device), txt_mask.to(device))\n",
    "    # Gets the word with the highest probability from the tokens\n",
    "    out = out.topk(1).indices.view(-1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f8c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Don't speak ill of others.\",\n",
    "\"To speak ill of others is a great crime.\",\n",
    "\"Rather rectify yourself through self-criticism.\",\n",
    "\"In this way, if you rectify yourself, others will follow you.\",\n",
    "\"To speak ill of others gives rise to more problems.\",\n",
    "\"This does not do any good to society.\",\n",
    "\"More than 80 percent people of our country live in villages.\",\n",
    "\"Most of them are poor and illiterate.\",\n",
    "\"Illiteracy is one of the causes of their poverty.\",\n",
    "\"Many of the villagers are landless cultivators.\",\n",
    "\"They cultivate the lands of other people throughout the year.\",\n",
    "\"They get a very small portion of the crops.\",\n",
    "\"They provide all of us with food.\",\n",
    "\"But in want they only starve.\",\n",
    "\"They suffer most.\",\n",
    "\"The situation needs to be changed.\",\n",
    "\"We live in the age of science.\",\n",
    "\"We can see the influence of science everywhere.\",\n",
    "\"Science is a constant companion of our life.\",\n",
    "\"We have made the impossible things possible with the help of science.\",\n",
    "\"Modern civilization is a contribution of science.\",\n",
    "\"Science should be devoted to the greater welfare of mankind.\",\n",
    "\"Rabindranath Tagore got the Nobel Prize in 1913 which is 98 years ago from today.\",\n",
    "\"He was awarded this prize for the translation of the Bengali 'Gitanjali' into English.\",\n",
    "\"This excellent rendering was the poet's own.\",\n",
    "\"In the English version of Gitanjali there are 103 songs.\"]\n",
    "\n",
    "dataset = MyDataset(text, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, collate_fn=data_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d2d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = tokenizer.vocab_size # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0cb886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 -> loss: 65.35458374023438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lj/8z00c58d6j5d5ylvfjwrhpk00000gn/T/ipykernel_49228/3008421808.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lj/8z00c58d6j5d5ylvfjwrhpk00000gn/T/ipykernel_49228/1667307004.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Makes the optimizer take a step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Prints the total loss and which step the model is on every 40 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             adamw(params_with_grad,\n\u001b[0m\u001b[1;32m    163\u001b[0m                   \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                   \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    220\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c78da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input: {}\".format(text[0]))\n",
    "pred_inp = tokenizer(\"[MASK] are an orange fruit.\", return_tensors='pt')\n",
    "out = predict(model, pred_inp['input_ids'])\n",
    "print(\"Output: {}\\n\".format(tokenizer.decode(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499378c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
