{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "040e57d0",
   "metadata": {},
   "source": [
    "- Tokenize input.\n",
    "- Create a label with input or clone input tensor.\n",
    "- Randomly masked some token in input.\n",
    "- Initialize the model and calculate the loss.\n",
    "- Finally update weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71137cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# See if there are any GPUs avaliable to train the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Retrieve the bert tokenizer from the transformers library\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_basic_tokenization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4354baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class definition for the storage of the Dataset\n",
    "class DatasetClass(Dataset):\n",
    "    def __init__(self, txt, tokenizer):\n",
    "        # Retrieves the text \n",
    "        self.txt = txt\n",
    "        # Retrieves the tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the length of the text\n",
    "        return len(self.txt)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Gets a specific line from the text\n",
    "        txt = self.txt[idx]\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785eb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class definition fot the positional encoding on an embedding of a token\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Sets the dropout rate to be the one passed in as a parameter\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Creates a tensor of zeros that has the shape of the model's dimensions\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # Creates an unsqeezed tensor with the position of the of the word within the sentence its in\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # Gets the sine value of the position of the word within the sentence its in\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Gets the cosine value of the position of the word within the sentence its in\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Sqeezes the positional encoding back to its original shape and returns the transpose for matrix multiplication\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # Saves the positional encoding\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, txt):\n",
    "        # Adds the positional encoding information to the text token\n",
    "        txt = txt + self.pe[:txt.size(0), :]\n",
    "        # Returns the forward ste and drops out random nodes in the network\n",
    "        return self.dropout(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "587dd842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class definition for the transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, num_input, heads, hidden, layers, dropout=0.5):\n",
    "        # Initializes the parameters od the model\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # Initializes model type\n",
    "        self.model_type = 'Transformer'\n",
    "        # Creates an instance of the positional encoder\n",
    "        self.pos_encoder = PositionalEncoding(num_input, dropout)\n",
    "        # Creates the encoder layers based on the tranmsformer hyperparameters\n",
    "        encoder_layers = TransformerEncoderLayer(num_input, heads, hidden, dropout, batch_first=True)\n",
    "        # Adds the encoder layers into the encoder\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, layers)\n",
    "        # Passes information about the inpot into the encoder\n",
    "        self.encoder = nn.Embedding(ntoken, num_input)\n",
    "        # Sets the number of inputs\n",
    "        self.num_input = num_input\n",
    "        # Creates the decorder layer using the Pytorch Linear function\n",
    "        self.decoder = nn.Linear(num_input, ntoken)\n",
    "        # Initialized the weights of the transformer model\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, size):\n",
    "        # Creates an attention mask with zeros across the diagonal\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        # Sets the values of the attention mask so that the current word the model is trying to predist is masked out\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Sets the range in which the values of the weights can occur\n",
    "        initrange = 0.1\n",
    "        # Sets the weights of the encoder\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # Zeros the weights of the decoder\n",
    "        self.decoder.bias.data.zero_()\n",
    "        # Sets the weights of the decoder\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, txt, txt_mask):\n",
    "        # Multiplies the information from the encoder by the square root of the number of inputs\n",
    "        txt = self.encoder(txt) * math.sqrt(self.num_input)\n",
    "        # Gets the information about the positional encoding of the current text being processed\n",
    "        txt = self.pos_encoder(txt)\n",
    "        # Gets the output to be input into the decoder using the encoder\n",
    "        output = self.transformer_encoder(txt, txt_mask)\n",
    "        # Decodes the processed output\n",
    "        output = self.decoder(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4beb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collate_fn(dataset_samples_list):\n",
    "    # Makes the dataset a numpy array instead of a python list\n",
    "    arr = np.array(dataset_samples_list)\n",
    "    \n",
    "    # Tokenizes the data using the BERT tokenizer\n",
    "    inputs = tokenizer(text=arr.tolist(), padding='max_length', max_length=6000, return_tensors='pt')\n",
    "    \n",
    "    # Returns the inputs to the model\n",
    "    return inputs\n",
    "\n",
    "def predict(model, input):\n",
    "    # Uses model.eval to get\n",
    "    model.eval()\n",
    "    # Generates a square subsequent attention mask for the example input\n",
    "    txt_mask = model.generate_square_subsequent_mask(input.size(1))\n",
    "    # Inputs the square subsequent mask of the input and the input into the model to predict all the masked tokens\n",
    "    out = model(input.to(device), txt_mask.to(device))\n",
    "    # Gets the word with the highest probability from the tokens\n",
    "    out = out.topk(1).indices.view(-1)\n",
    "    return out\n",
    "\n",
    "def mask_random(model, batch, token_list, mask_token, mask_percent=0.15):\n",
    "    # Gets the input ids from the batch of the dataset\n",
    "    input = batch['input_ids'].clone()\n",
    "    # Generates a square subsequent mask of the batch of the dataset\n",
    "    txt_mask = model.generate_square_subsequent_mask(batch['input_ids'].size(1))\n",
    "    # Creates an array the size of input id's with random values between 0 and 1\n",
    "    rand_value = torch.rand(batch.input_ids.shape)\n",
    "    # Creates a random masking matrix where only values with a vlaue lower then the set percentage are kept (15%)\n",
    "    rand_mask = (rand_value < mask_percent)\n",
    "    # Adds parameters to the random mask to make sure special tokens do not get masked\n",
    "    for token in token_list:\n",
    "        rand_mask = rand_mask * (input != token)\n",
    "    # Gets the indexes of all of the tokens to be masked from the dataset\n",
    "    mask_idx = (rand_mask.flatten() == True).nonzero().view(-1)\n",
    "    # The inputs are flattened for multiplication\n",
    "    input = input.flatten()\n",
    "    # Sets which token is the [MASK] token (103 in this case)\n",
    "    input[mask_idx] = mask_token\n",
    "    # Sets the input to be the size of the input id's of the current batch \n",
    "    input = input.view(batch['input_ids'].size())\n",
    "    \n",
    "    return input, txt_mask\n",
    "\n",
    "def train(model, dataloader, epochs=500):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # Sets how loss will be calcuated using Cross Entropy\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Sets the optimizer to be AdamW\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # Runs the model on the designated number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Separates the data into batches\n",
    "        for batch in dataloader:\n",
    "            # Zeroes the gradient used for gradient descent\n",
    "            optimizer.zero_grad()\n",
    "            # Sets what the mask token is\n",
    "            mask_token = 103\n",
    "            # Sets what the special tokens are\n",
    "            token_list = [101, 102, 0]\n",
    "            input, txt_mask = mask_random(model, batch, token_list, mask_token, 0.15)\n",
    "            # Gets the output by entering the masked input and unmasked input into the model infrastructure\n",
    "            out = model(input.to(device), txt_mask.to(device))\n",
    "            # Calculated the loss of the model (-log p(word|sentence))\n",
    "            loss = criterion(out.view(-1, vocab_size), batch['input_ids'].view(-1).to(device))\n",
    "            # Adds the loss of each batch to the total loss\n",
    "            total_loss += loss\n",
    "            # Does backward propagation\n",
    "            loss.backward()\n",
    "            # Makes the optimizer take a step\n",
    "            optimizer.step()\n",
    "    \n",
    "        # Prints the total loss and which step the model is on every 40 epochs\n",
    "        print(\"Epoch: {} -> loss: {}\".format(epoch+1, total_loss/(len(dataloader)*epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f8c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset rotten_tomatoes (/Users/dominikzeman/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a18f0cf4d24d8cbbfe2e2c3ee4c056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_text = dataset['train']['text']\n",
    "\n",
    "for i, sentence in enumerate(train_text):\n",
    "    new_sentence = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", sentence)\n",
    "    train_text[i] = new_sentence\n",
    "    \n",
    "train_text = train_text[: -3530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eaa7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetClass(train_text, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=data_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d2d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of vocabulary\n",
    "vocab_size = tokenizer.vocab_size\n",
    "# The number of epochs the model needs to train for\n",
    "epochs = 100\n",
    "# The size of the embedding dimension\n",
    "embedding = 100\n",
    "# The dimension of the hidden layer in the network\n",
    "hidden = 200\n",
    "# The number of Transformer layers in the network\n",
    "layers = 2\n",
    "# The number of heads in the models for multihead attention\n",
    "heads = 2\n",
    "# The percentage of nodes that will drop out to prevent overfitting \n",
    "dropout = 0.2\n",
    "# Creates an instance of the model with the specified hyperparameters\n",
    "model = TransformerModel(vocab_size, embedding, heads, hidden, layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f0cb886",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lj/8z00c58d6j5d5ylvfjwrhpk00000gn/T/ipykernel_37080/740760735.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Trains the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lj/8z00c58d6j5d5ylvfjwrhpk00000gn/T/ipykernel_37080/3947880866.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, epochs)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Gets the output by entering the masked input and unmasked input into the model infrastructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;31m# Calculated the loss of the model (-log p(word|sentence))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lj/8z00c58d6j5d5ylvfjwrhpk00000gn/T/ipykernel_37080/4079106216.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, txt, txt_mask)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Decodes the processed output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Trains the model\n",
    "train(model, dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the trained masked model\n",
    "torch.save(model, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c78da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the trained model\n",
    "#model = torch.load('models')\n",
    "\n",
    "print(\"Input: {}\".format(text[0]))\n",
    "pred_inp = tokenizer(\"Illiterate [MASK] get in the way of science.\", return_tensors='pt')\n",
    "out = predict(model, pred_inp['input_ids'])\n",
    "print(\"Output: {}\\n\".format(tokenizer.decode(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc705f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
