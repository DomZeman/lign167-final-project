{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "040e57d0",
   "metadata": {},
   "source": [
    "- Tokenize input.\n",
    "- Create a label with input or clone input tensor.\n",
    "- Randomly masked some token in input.\n",
    "- Initialize the model and calculate the loss.\n",
    "- Finally update weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71137cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# See if there are any GPUs avaliable to train the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Retrieve the bert tokenizer from the transformers library\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_basic_tokenization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4354baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer):\n",
    "        # Retrieves the text \n",
    "        self.txt = txt\n",
    "        # Retrieves the tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the length of the text\n",
    "        return len(self.txt)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Gets a specific line from the text\n",
    "        txt = self.txt[idx]\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "785eb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "587dd842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, heads, hidden, layers, dropout=0.5):\n",
    "        # Initializes the parameters od the model\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # Initializes model type\n",
    "        self.model_type = 'Transformer'\n",
    "        # Gets the positional encoder \n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        # Sets the laters based on the hyperparameters\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, heads, hidden, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, layers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, size):\n",
    "        # Creates an attention mask with zeros across the diagonal\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initializes the weights of the model between the modes before training the dataset\n",
    "        # Sets the range in which the values of the weights can occur\n",
    "        initrange = 0.1\n",
    "        # Sets the weights of the encoder\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # Zeros the weights of the decoder\n",
    "        self.decoder.bias.data.zero_()\n",
    "        # Sets the weights of the decoder\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, txt, txt_mask):\n",
    "        txt = self.encoder(txt) * math.sqrt(self.ninp)\n",
    "        txt = self.pos_encoder(txt)\n",
    "        output = self.transformer_encoder(txt, txt_mask)\n",
    "        # Decodes the processed output\n",
    "        output = self.decoder(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ea4beb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collate_fn(dataset_samples_list):\n",
    "    # Makes the dataset a numpy array instead of a python list\n",
    "    arr = np.array(dataset_samples_list)\n",
    "    \n",
    "    # Tokenizes the data using the BERT tokenizer\n",
    "    inputs = tokenizer(text=arr.tolist(), padding='max_length', max_length=30, return_tensors='pt')\n",
    "    \n",
    "    # Returns the inputs to the model\n",
    "    return inputs\n",
    "\n",
    "def predict(model, input):\n",
    "    # Uses model.eval to get\n",
    "    model.eval()\n",
    "    # Generates a square subsequent mask for the example input\n",
    "    txt_mask = model.generate_square_subsequent_mask(input.size(1))\n",
    "    # Inputs the square subsequent mask of the input and the input into the model\n",
    "    out = model(input.to(device), txt_mask.to(device))\n",
    "    # Gets the word with the highest probability from the tokens\n",
    "    out = out.topk(1).indices.view(-1)\n",
    "    return out\n",
    "\n",
    "def mask_random(model, batch, token_list, mask_token, mask_percent=0.15):\n",
    "    # Gets the input ids from the batch of the dataset\n",
    "    input = batch['input_ids'].clone()\n",
    "    # Generates a square subsequent mask of the batch of the dataset\n",
    "    txt_mask = model.generate_square_subsequent_mask(batch['input_ids'].size(1))\n",
    "    # Creates an array the size of input id's to mask out 15% of the dataset\n",
    "    rand_value = torch.rand(batch.input_ids.shape)\n",
    "    # Adds parameters to the random mask to make sure special tokens do not get masked\n",
    "    rand_mask = (rand_value < mask_percent)\n",
    "    \n",
    "    for token in token_list:\n",
    "        rand_mask = rand_mask * (input != token)\n",
    "    \n",
    "    # Gets the indexes of all of the tokens to be masked from the dataset\n",
    "    mask_idx = (rand_mask.flatten() == True).nonzero().view(-1)\n",
    "    # The inputs are flattened for multiplication\n",
    "    input = input.flatten()\n",
    "    # Sets which token is the [MASK] token (103)\n",
    "    input[mask_idx] = mask_token\n",
    "    input = input.view(batch['input_ids'].size())\n",
    "    \n",
    "    return input, txt_mask\n",
    "\n",
    "def train(model, dataloader, epochs=500):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # Sets how loss will be calcuated using Cross Entropy\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Sets the optimizer to be AdamW\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            # Zeroes the gradient used for gradient descent\n",
    "            optimizer.zero_grad()\n",
    "            mask_token = 103\n",
    "            token_list = [101, 102, 0]\n",
    "            input, txt_mask = mask_random(model, batch, token_list, mask_token, 0.15)\n",
    "\n",
    "            # Gets the output by entering the masked input and unmasked input into the model infrastructure\n",
    "            out = model(input.to(device), txt_mask.to(device))\n",
    "            # Calculated the loss of the model (-log p(word|sentence))\n",
    "            loss = criterion(out.view(-1, vocab_size), batch['input_ids'].view(-1).to(device))\n",
    "            # Adds the loss of each batch to the total loss\n",
    "            total_loss += loss\n",
    "            # Does backward propagation\n",
    "            loss.backward()\n",
    "            # Makes the optimizer take a step\n",
    "            optimizer.step()\n",
    "    \n",
    "        # Prints the total loss and which step the model is on every 40 epochs\n",
    "        if (epoch+1)%40==0 or epoch==0:\n",
    "            print(\"Epoch: {} -> loss: {}\".format(epoch+1, total_loss/(len(dataloader)*epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4f8c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Don't speak ill of others.\",\n",
    "\"To speak ill of others is a great crime.\",\n",
    "\"Rather rectify yourself through self-criticism.\",\n",
    "\"In this way, if you rectify yourself, others will follow you.\",\n",
    "\"To speak ill of others gives rise to more problems.\",\n",
    "\"This does not do any good to society.\",\n",
    "\"More than 80 percent people of our country live in villages.\",\n",
    "\"Most of them are poor and illiterate.\",\n",
    "\"Illiteracy is one of the causes of their poverty.\",\n",
    "\"Many of the villagers are landless cultivators.\",\n",
    "\"They cultivate the lands of other people throughout the year.\",\n",
    "\"They get a very small portion of the crops.\",\n",
    "\"They provide all of us with food.\",\n",
    "\"But in want they only starve.\",\n",
    "\"They suffer most.\",\n",
    "\"The situation needs to be changed.\",\n",
    "\"We live in the age of science.\",\n",
    "\"We can see the influence of science everywhere.\",\n",
    "\"Science is a constant companion of our life.\",\n",
    "\"We have made the impossible things possible with the help of science.\",\n",
    "\"Modern civilization is a contribution of science.\",\n",
    "\"Science should be devoted to the greater welfare of mankind.\",\n",
    "\"Rabindranath Tagore got the Nobel Prize in 1913 which is 98 years ago from today.\",\n",
    "\"He was awarded this prize for the translation of the Bengali 'Gitanjali' into English.\",\n",
    "\"This excellent rendering was the poet's own.\",\n",
    "\"In the English version of Gitanjali there are 103 songs.\"]\n",
    "\n",
    "dataset = MyDataset(text, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, collate_fn=data_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "49d2d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size # the size of vocabulary\n",
    "epochs = 400\n",
    "embedding = 200 # embedding dimension\n",
    "hidden = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "layers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "heads = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(vocab_size, embedding, heads, hidden, layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9f0cb886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 -> loss: 69.1144790649414\n",
      "Epoch: 40 -> loss: 3.2742581367492676\n",
      "Epoch: 80 -> loss: 2.191288471221924\n",
      "Epoch: 120 -> loss: 1.6457726955413818\n",
      "Epoch: 160 -> loss: 1.3164350986480713\n",
      "Epoch: 200 -> loss: 1.1011258363723755\n",
      "Epoch: 240 -> loss: 0.9527290463447571\n",
      "Epoch: 280 -> loss: 0.8440445065498352\n",
      "Epoch: 320 -> loss: 0.7595718502998352\n",
      "Epoch: 360 -> loss: 0.6913173198699951\n",
      "Epoch: 400 -> loss: 0.635842502117157\n"
     ]
    }
   ],
   "source": [
    "train(model, dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b4dc6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "57c78da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Don't speak ill of others.\n",
      "Output: [CLS] illiterate is get in the way of science. [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model = torch.load('models')\n",
    "#model.eval()\n",
    "\n",
    "print(\"Input: {}\".format(text[0]))\n",
    "pred_inp = tokenizer(\"Illiterate [MASK] get in the way of science.\", return_tensors='pt')\n",
    "out = predict(model, pred_inp['input_ids'])\n",
    "print(\"Output: {}\\n\".format(tokenizer.decode(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc705f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
